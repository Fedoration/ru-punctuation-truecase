{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python D:/progamming/va/truecase/ru-punctuation-truecase/src/prepare_data.py --data_dir D:/progamming/va/truecase/ru-punctuation-truecase/data/tatoeba_dataset --num_samples -1 --percent_dev 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 /Users/falaputin/virtual_assistant/trucase_project/ru-punctuation-truecase/src/prepare_data.py --data_dir /Users/falaputin/virtual_assistant/trucase_project/ru-punctuation-truecase/data/tatoeba_dataset --num_samples -1 --percent_dev 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-truecase\\\\src')\n",
    "from process_text import clean_text, clean_text_3times\n",
    "\n",
    "# ========== Data global variables ==========\n",
    "PATH_TO_DATA = \"../data\"\n",
    "\n",
    "# ========== Model global variables ==========\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased-conversational\"\n",
    "# \"DeepPavlov/rubert-base-cased-conversational\" -> rubert-base-cased-conversational\n",
    "SHORT_MODEL_NAME = MODEL_NAME.split('/')[1] if '/' in MODEL_NAME else MODEL_NAME\n",
    "MODEL_MAX_LENGTH = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для hugging face transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка собственного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2array(path_to_file: str) -> List[List[str]]:\n",
    "    result = []\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            result.append(line.split(' '))\n",
    "    return result\n",
    "\n",
    "\n",
    "def textlabel2arrays(path_to_text: str, path_to_labels: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(path_to_text, 'r') as f_text:\n",
    "        with open(path_to_labels, 'r') as f_labels:\n",
    "            for line_text, line_labels in zip(f_text.readlines(), f_labels.readlines()):\n",
    "\n",
    "                line_text = line_text.strip()\n",
    "                line_labels = line_labels.strip()\n",
    "                \n",
    "                texts.append(line_text.split(' '))\n",
    "                labels.append(line_labels.split(' '))\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def encode_tags(tags, tag2id, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def save_to_pickle(filename: str, data):\n",
    "    # If file exists, delete it.\n",
    "    if os.path.isfile(filename):\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print(\"Error: %s file not found\" % filename)\n",
    "\n",
    "    with open(f'{filename}', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_from_pickle(filename: str):\n",
    "    data = None\n",
    "    if os.path.isfile(filename):\n",
    "        with open(f'{filename}', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "    else:\n",
    "        print(\"Error: %s file not found\" % filename)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_text = \"../data/tatoeba_dataset/text_train.txt\"\n",
    "path_to_train_labels = \"../data/tatoeba_dataset/labels_train.txt\"\n",
    "path_to_val_text = \"../data/tatoeba_dataset/text_dev.txt\"\n",
    "path_to_val_labels = \"../data/tatoeba_dataset/labels_dev.txt\"\n",
    "\n",
    "train_texts, train_tags = textlabel2arrays(path_to_train_text, path_to_train_labels)\n",
    "val_texts, val_tags = textlabel2arrays(path_to_val_text, path_to_val_labels)\n",
    "\n",
    "unique_labels = set(tag for doc in train_tags for tag in doc)\n",
    "label_names = list(unique_labels)\n",
    "label2id = {tag: id for id, tag in enumerate(label_names)}\n",
    "id2label = {id: tag for tag, id in label2id.items()}\n",
    "\n",
    "print(len(train_texts) == len(train_tags))\n",
    "print(len(val_texts) == len(val_tags))\n",
    "print(len(train_texts), len(train_tags))\n",
    "print(len(val_texts), len(val_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate encodings, labels and save it to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "\n",
    "train_labels = encode_tags(train_tags, label2id, train_encodings)\n",
    "val_labels = encode_tags(val_tags, label2id, val_encodings)\n",
    "\n",
    "# save train_encodings\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/train_encodings_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "save_to_pickle(pickle_filename, train_encodings)\n",
    "\n",
    "# save val_encodings\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/val_encodings_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "save_to_pickle(pickle_filename, val_encodings)\n",
    "\n",
    "# save train_labels\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/train_labels_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "save_to_pickle(pickle_filename, train_labels)\n",
    "\n",
    "# save val_labels\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/val_labels_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "save_to_pickle(pickle_filename, val_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load encodings, labels from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_encodings\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/train_encodings_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "train_encodings = load_from_pickle(pickle_filename)\n",
    "\n",
    "# load val_encodings\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/val_encodings_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "val_encodings = load_from_pickle(pickle_filename)\n",
    "\n",
    "# load train_labels\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/train_labels_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "train_labels = load_from_pickle(pickle_filename)\n",
    "\n",
    "# load val_labels\n",
    "pickle_filename = f\"{PATH_TO_DATA}/cached/val_labels_{SHORT_MODEL_NAME}_{MODEL_MAX_LENGTH}.pickle\"\n",
    "val_labels = load_from_pickle(pickle_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitalizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\")\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = CapitalizationDataset(train_encodings, train_labels)\n",
    "val_dataset = CapitalizationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(label_names), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"huggingface-punctuation-and-capitalization\",\n",
    "           name=f\"{SHORT_MODEL_NAME}-{MODEL_MAX_LENGTH}-{'tatoeba_dataset'}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=128,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[tag2id[tag] for tag in doc] for doc in train_tags]\n",
    "encoded_labels = []\n",
    "idx = 0\n",
    "for doc_labels, doc_offset in zip(labels, train_encodings.offset_mapping):\n",
    "    # create an empty array of -100\n",
    "    doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "    arr_offset = np.array(doc_offset)\n",
    "    # print(doc_enc_labels)\n",
    "    # print(len(arr_offset))\n",
    "\n",
    "    # set labels whose first offset position is 0 and the second is not 0\n",
    "    try:\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "    except Exception as e:\n",
    "        print(idx)\n",
    "        \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_texts_128 = [36934, 80799, 80800, 99989, 119863, 126620, 130799, 131430, 139275, 160289]\n",
    "broken_texts_512 = [126620, 130799, 160289]\n",
    "for idx in broken_texts_128:\n",
    "    print(val_texts[idx])\n",
    "    print(len(val_texts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(train_encodings.tokens(), train_labels[0]):\n",
    "    print(token, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
