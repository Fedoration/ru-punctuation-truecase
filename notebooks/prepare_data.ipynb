{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 /Users/falaputin/virtual_assistant/ru-punctuation-truecase/src/prepare_data.py --data_dir /Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/tatoeba_dataset --num_samples -1 --percent_dev 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/falaputin/Downloads/cleaned_top300k_merged_database.parquet.gzip\", engine=\"pyarrow\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для hugging face transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts, tags = read_wnut('/Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/wnut17train.conll')\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)\n",
    "\n",
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings.tokens(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] -100\n",
      "R 12\n",
      "##T -100\n",
      "@ 12\n",
      "B -100\n",
      "##ie -100\n",
      "##bers -100\n",
      "##F -100\n",
      "##air -100\n",
      "##ies -100\n",
      ": 12\n",
      "the 12\n",
      "day 12\n",
      "Justin 8\n",
      "B 3\n",
      "##ie -100\n",
      "##ber -100\n",
      "was 12\n",
      "born 12\n",
      "was 12\n",
      "a 12\n",
      "rainy 12\n",
      "day 12\n",
      ", 12\n",
      "but 12\n",
      "it 12\n",
      "was 12\n",
      "no 12\n",
      "rain 12\n",
      ", 12\n",
      "NO 12\n",
      "! 12\n",
      "the 12\n",
      "heaven 12\n",
      "was 12\n",
      "crying 12\n",
      "cause 12\n",
      "he 12\n",
      "lost 12\n",
      "his 12\n",
      "most 12\n",
      ". 12\n",
      ". -100\n",
      ". -100\n",
      "[SEP] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n",
      "[PAD] -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(train_encodings.tokens(), train_labels[0]):\n",
    "    print(token, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка собственного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2array(path_to_file: str) -> List[List[str]]:\n",
    "    result = []\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            result.append(line.split(' '))\n",
    "    return result\n",
    "\n",
    "\n",
    "def textlabel2arrays(path_to_text: str, path_to_labels: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(path_to_text, 'r') as f_text:\n",
    "        with open(path_to_labels, 'r') as f_labels:\n",
    "            for line_text, line_labels in zip(f_text.readlines(), f_labels.readlines()):\n",
    "\n",
    "                line_text = line_text.strip()\n",
    "                line_labels = line_labels.strip()\n",
    "                \n",
    "                texts.append(line_text.split(' '))\n",
    "                labels.append(line_labels.split(' '))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "path_to_train_text = \"/Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/tatoeba_dataset/text_train.txt\"\n",
    "path_to_train_labels = \"/Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/tatoeba_dataset/labels_train.txt\"\n",
    "path_to_val_text = \"/Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/tatoeba_dataset/text_dev.txt\"\n",
    "path_to_val_labels = \"/Users/falaputin/virtual_assistant/ru-punctuation-truecase/data/tatoeba_dataset/labels_dev.txt\"\n",
    "\n",
    "train_texts, train_tags = textlabel2arrays(path_to_train_text, path_to_train_labels)\n",
    "val_texts, val_tags = textlabel2arrays(path_to_val_text, path_to_val_labels)\n",
    "\n",
    "unique_tags = set(tag for doc in train_tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "print(len(train_texts) == len(train_tags))\n",
    "print(len(val_texts) == len(val_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806458 806458\n",
      "201614 201614\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(train_tags))\n",
    "print(len(val_texts), len(val_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-conversational')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings.tokens(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
