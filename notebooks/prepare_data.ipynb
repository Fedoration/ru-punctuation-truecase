{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python D:/progamming/va/truecase/ru-punctuation-truecase/src/prepare_data.py --data_dir D:/progamming/va/truecase/ru-punctuation-truecase/data/tatoeba_dataset --num_samples -1 --percent_dev 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'D:/progamming/va/truecase/ru-punctuation-truecase/src')\n",
    "from process_text import clean_text, clean_text_3times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/falaputin/Downloads/cleaned_top300k_merged_database.parquet.gzip\", engine=\"pyarrow\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для hugging face transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts, tags = read_wnut('D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-truecase\\\\data\\\\wnut17train.conll')\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)\n",
    "\n",
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings.tokens(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(train_encodings.tokens(), train_labels[0]):\n",
    "    print(token, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка собственного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_3times(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    text = clean_text(text)\n",
    "    text = clean_text(text)\n",
    "    return text\n",
    "\n",
    "def file2array(path_to_file: str) -> List[List[str]]:\n",
    "    result = []\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            result.append(line.split(' '))\n",
    "    return result\n",
    "\n",
    "\n",
    "def textlabel2arrays(path_to_text: str, path_to_labels: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(path_to_text, 'r') as f_text:\n",
    "        with open(path_to_labels, 'r') as f_labels:\n",
    "            for line_text, line_labels in zip(f_text.readlines(), f_labels.readlines()):\n",
    "\n",
    "                line_text = line_text.strip()\n",
    "                line_labels = line_labels.strip()\n",
    "                \n",
    "                texts.append(line_text.split(' '))\n",
    "                labels.append(line_labels.split(' '))\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_text = \"../data/tatoeba_dataset/text_train.txt\"\n",
    "path_to_train_labels = \"../data/tatoeba_dataset/labels_train.txt\"\n",
    "path_to_val_text = \"../data/tatoeba_dataset/text_dev.txt\"\n",
    "path_to_val_labels = \"../data/tatoeba_dataset/labels_dev.txt\"\n",
    "\n",
    "train_texts, train_tags = textlabel2arrays(path_to_train_text, path_to_train_labels)\n",
    "val_texts, val_tags = textlabel2arrays(path_to_val_text, path_to_val_labels)\n",
    "\n",
    "unique_tags = set(tag for doc in train_tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "print(len(train_texts) == len(train_tags))\n",
    "print(len(val_texts) == len(val_tags))\n",
    "print(len(train_texts), len(train_tags))\n",
    "print(len(val_texts), len(val_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-conversational', model_max_length=512)\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(val_texts[99989])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode_chars(input_str):\n",
    "    string_encode = input_str.encode(\"ascii\", \"ignore\")\n",
    "    string_decode = string_encode.decode()\n",
    "    return string_decode\n",
    "\n",
    "s = ' '.join(['\\u200bникто', 'не', 'знал', 'настоящую', 'глубину', 'озера'])\n",
    "# s = ' '.join(val_texts[36934])\n",
    "# s = 'почему дзюдо'\n",
    "remove_unicode_chars('\\u200bникто')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_unicode_chars(input_str):\n",
    "    pattern = re.compile(u\"[\\u200c-\\u200f\\u202a-\\u202f\\u2066-\\u2069-\\u200b]\")\n",
    "    input_str = pattern.sub(\"\", input_str)\n",
    "    input_str = input_str.replace('\\xad', '')\n",
    "    input_str = input_str.replace('  ', ' ')\n",
    "    return input_str.strip()\n",
    "\n",
    "s = ' '.join(['почему', 'дзюдо', '\\xad', 'лучший', 'выбор', 'для', 'вашего', 'ребёнка'])\n",
    "remove_unicode_chars(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'почему дзюдо \\xad лучший выбор для вашего ребёнка'\n",
    " \n",
    "#encode() method\n",
    "strencode = s.encode(\"ascii\", \"ignore\")\n",
    " \n",
    "#decode() method\n",
    "strdecode = strencode.decode()\n",
    "strdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"почему дзюдо ­ лучший выбор для вашего ребёнка\"\n",
    "s = ' '.join(['\\u200bникто', 'не', 'знал', 'настоящую', 'глубину', 'озера'])\n",
    "clean_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[tag2id[tag] for tag in doc] for doc in val_tags]\n",
    "encoded_labels = []\n",
    "idx = 0\n",
    "for doc_labels, doc_offset in zip(labels, val_encodings.offset_mapping):\n",
    "    # create an empty array of -100\n",
    "    doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "    arr_offset = np.array(doc_offset)\n",
    "    # print(doc_enc_labels)\n",
    "    # print(len(arr_offset))\n",
    "\n",
    "    # set labels whose first offset position is 0 and the second is not 0\n",
    "    try:\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "    except Exception as e:\n",
    "        print(idx)\n",
    "        \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'set labels whose first offset position is 0 and the second is not 0'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_texts_128 = [36934, 80799, 80800, 99989, 119863, 126620, 130799, 131430, 139275, 160289]\n",
    "broken_texts_512 = [126620, 130799, 160289]\n",
    "for idx in broken_texts_128:\n",
    "    print(val_texts[idx])\n",
    "    print(len(val_texts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(train_encodings.tokens(), train_labels[0]):\n",
    "    print(token, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
