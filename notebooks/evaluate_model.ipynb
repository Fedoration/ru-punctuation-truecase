{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-truecase\\\\src')\n",
    "from process_text import clean_text, clean_text_3times\n",
    "\n",
    "# ========== Data global variables ==========\n",
    "PATH_TO_DATA = \"../data\"\n",
    "\n",
    "# ========== Model global variables ==========\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased-conversational\"\n",
    "# \"DeepPavlov/rubert-base-cased-conversational\" -> rubert-base-cased-conversational\n",
    "SHORT_MODEL_NAME = MODEL_NAME.split('/')[1] if '/' in MODEL_NAME else MODEL_NAME\n",
    "MODEL_MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, tag2id, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, label_names):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts_into_words(texts: List[str]):\n",
    "    texts_words = []\n",
    "    for text in texts:\n",
    "        texts_words.append(text.split(' '))\n",
    "    return texts_words\n",
    "\n",
    "\n",
    "def tokenize_texts(texts_words: List[List[str]], tokenizer):\n",
    "    inputs = tokenizer(texts_words, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def predict_token_classification(inputs, token_classification_model):\n",
    "    with torch.no_grad():\n",
    "        logits = token_classification_model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def restore_capitalization(texts: List[str], tokenizer, token_classification_model) -> List[str]:\n",
    "    texts_words = split_texts_into_words(texts)\n",
    "    inputs = tokenize_texts(texts_words, tokenizer)\n",
    "    predictions = predict_token_classification(inputs, token_classification_model)\n",
    "\n",
    "    truecase_texts = []\n",
    "    for text, model_input, predict in zip(texts_words, inputs.encodings, predictions):\n",
    "        predicted_token_class = [model.config.id2label[t.item()] for t in predict]\n",
    "        \n",
    "        word_class = {}\n",
    "        for word_id, token_class in zip(model_input.word_ids, predicted_token_class):\n",
    "            if (word_id != None) and (not word_id in word_class):\n",
    "                word_class[word_id] = token_class\n",
    "        \n",
    "        truecase_words = []\n",
    "        for i, word in enumerate(text):\n",
    "            is_upper = word_class[i] == 'U'\n",
    "            if is_upper:\n",
    "                truecase_word = word.capitalize()\n",
    "            else:\n",
    "                truecase_word = word\n",
    "            \n",
    "            truecase_words.append(truecase_word)\n",
    "\n",
    "        truecase_texts.append(' '.join(truecase_words))\n",
    "    return truecase_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    (\"меня зовут сергей а как тебя\", \"Меня зовут Сергей а как тебя\"),\n",
    "    (\"подскажи пожалуйста сегодня вторник или среда\", \"Подскажи пожалуйста сегодня вторник или среда\"),\n",
    "    (\"закрой за мной дверь я ухожу\", \"Закрой за мной дверь я ухожу\"),\n",
    "    (\"в каком году родилась алла пугачёва\", \"В каком году родилась Алла Пугачёва\"),\n",
    "    (\"когда родилась алла пугачёва\", \"Когда родилась Алла Пугачёва\"),\n",
    "    (\"когда родилась пугачёва\", \"Когда родилась Пугачёва\"),\n",
    "    (\"скажи год рождения пугачевой\", \"Скажи год рождения Пугачевой\"),\n",
    "    (\"год рождения аллы пугачевой\", \"Год рождения Аллы Пугачевой\"),\n",
    "    (\"год рождения пугачевой\", \"Год рождения Пугачевой\"),\n",
    "    (\"день рождения пугачевой\", \"День рождения Пугачевой\"),\n",
    "    (\"когда день рождения у аллы пугачевой\", \"Когда день рождения у Аллы Пугачевой\"),\n",
    "    (\"в каком году родилась алла пугачева\", \"В каком году родилась Алла Пугачева\"),\n",
    "    (\"год рождения пугачевой\", \"Год рождения Пугачевой\"),\n",
    "    (\"иван пятый это кто\", \"Иван Пятый это кто\"),\n",
    "    (\"кто такой иван пятый\", \"Кто такой Иван Пятый\"),\n",
    "    (\"кто такой иван пятый\", \"Кто такой Иван Пятый\"),\n",
    "    (\"что такое лыжи\", \"Что такое лыжи\"),\n",
    "    (\"швеция столица\", \"Швеция столица\"),\n",
    "    (\"столица швеции\", \"столица Швеции\"),\n",
    "    (\"какая столица швеции\", \"Какая столица Швеции\"),\n",
    "    (\"как называется столица у швеции\", \"Как называется столица у Швеции\"),\n",
    "    (\"расскажи когда родился пушкин\", \"Расскажи когда родился Пушкин\"),\n",
    "    (\"когда дата рождения пушкина\", \"Когда дата рождения Пушкина\"),\n",
    "    (\"скажи дату рождения пушкина\", \"Скажи дату рождения Пушкина\"),\n",
    "    (\"сколько прожил пушкин\", \"Сколько прожил Пушкин\"),\n",
    "    (\"когда родился пушкин\", \"Когда родился Пушкин\"),\n",
    "    (\"когда родился александр сергеевич пушкин\", \"Когда родился Александр Сергеевич Пушкин\"),\n",
    "    (\"когда родился александр пушкин\", \"Когда родился Александр Пушкин\"),\n",
    "    (\"кто такой лев николаевич толстой\", \"Кто такой Лев Николаевич Толстой\"),\n",
    "    (\"кто такой лев толстой\", \"Кто такой Лев Толстой\"),\n",
    "    (\"сколько лет путину\", \"Сколько лет Путину\"),\n",
    "    (\"возраст путина\", \"Возраст Путина\"),\n",
    "    (\"какой возраст у владимира путина\", \"Какой возраст у Владимира Путина\"),\n",
    "    (\"какой возраст у путина\", \"Какой возраст у Путина\"),\n",
    "    (\"какой возраст у владимира владимировича путина\", \"Какой возраст у Владимира Владимировича Путина\"),\n",
    "    (\"праздник благовещение какого числа\", \"Праздник Благовещение какого числа\"),\n",
    "    (\"что такое благовещение пресвятой богородицы\", \"Что такое Благовещение Пресвятой Богородицы\"),\n",
    "    (\"кому принадлежит компания газпром\", \"Кому принадлежит компания Газпром\"),\n",
    "    (\"кто директор газпрома\", \"Кто директор Газпрома\"),\n",
    "    (\"кто является директором компании газпром\", \"Кто является директором компании Газпром\"),\n",
    "    (\"сколько лет москве\", \"Сколько лет Москве\"),\n",
    "    (\"сколько лет городу москва\", \"Сколько лет городу Москва\"),\n",
    "    (\"в каком году умер брежнев\", \"В каком году умер Брежнев\"),\n",
    "    (\"в каком году скончался леонид брежнев\", \"В каком году скончался Леонид Брежнев\"),\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "\n",
    "checkpoint = 'D:/progamming/va/truecase/ru-punctuation-truecase/src/results/rubert-base-cased-conversational-512-tatoeba_dataset/20-53-40/checkpoint-31505/'\n",
    "# checkpoint = \"D:/progamming/va/truecase/ru-punctuation-truecase/src/results/checkpoint-18903/\"\n",
    "# model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-capitalization\\\\rubert-base-cased-conversational-512-tatoeba_dataset\\\\02-09-2023-11-01-00\\\\checkpoint-7482\")\n",
    "\n",
    "inference_results = restore_capitalization([q[0] for q in test_queries], tokenizer, model)\n",
    "for query, result in zip(test_queries, inference_results):\n",
    "    print(f'Query   : {query[0]}')\n",
    "    print(f'Combined: {result.strip()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capitalization_mask(text):\n",
    "    return [int(ch.isupper()) for ch in text]\n",
    "\n",
    "def capitalization_metrics_report(y_true, y_predict):\n",
    "    scores_by_sentences = []\n",
    "    scores_by_characters = {\n",
    "        'f1_score': [],\n",
    "        'accuracy_score': []\n",
    "    }\n",
    "    for true_text, predicted_text in zip(y_true, y_predict):\n",
    "        true_text = true_text.strip()\n",
    "        predicted_text = predicted_text.strip()\n",
    "\n",
    "        true_text_mask = get_capitalization_mask(true_text)\n",
    "        predicted_text_mask = get_capitalization_mask(predicted_text)\n",
    "        \n",
    "        # scores by characters\n",
    "        f1score = f1_score(true_text_mask, predicted_text_mask)\n",
    "        accuracy = accuracy_score(true_text_mask, predicted_text_mask)\n",
    "        scores_by_characters['f1_score'].append(f1score)\n",
    "        scores_by_characters['accuracy_score'].append(accuracy)\n",
    "\n",
    "        # scores by sentences\n",
    "        scores_by_sentences.append(int(true_text == predicted_text))\n",
    "    \n",
    "    mean_accuracy_score_by_sentences = np.mean(scores_by_sentences)\n",
    "    mean_f1_score_by_characters = np.mean(scores_by_characters['f1_score'])\n",
    "    mean_accuracy_score_by_characters = np.mean(scores_by_characters['accuracy_score'])\n",
    "\n",
    "    return mean_accuracy_score_by_sentences, mean_f1_score_by_characters, mean_accuracy_score_by_characters\n",
    "\n",
    "data = {\n",
    "    'checkpoint': [],\n",
    "    'accuracy_sentences': [],\n",
    "    'f1_characters': [],\n",
    "    'accuracy_characters': []\n",
    "}\n",
    "path_to_runs = \"D:/progamming/va/truecase/ru-punctuation-capitalization/rubert-base-cased-conversational-512-tatoeba_dataset\"\n",
    "for run in os.listdir(path_to_runs):\n",
    "    print(f\"------------------------ RUN\\t{run} ------------------------\\n\")\n",
    "    path_to_run = os.path.join(path_to_runs, run)\n",
    "    for checkpoint in os.listdir(path_to_run):\n",
    "        path_to_checkpoint = os.path.join(path_to_run, checkpoint) \n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(path_to_checkpoint)\n",
    "\n",
    "        inference_results = restore_capitalization([q[0] for q in test_queries], tokenizer, model)\n",
    "\n",
    "        print(path_to_checkpoint)\n",
    "        true_texts = [q[1] for q in test_queries]\n",
    "        mean_accuracy_score_by_sentences, mean_f1_score_by_characters, mean_accuracy_score_by_characters = capitalization_metrics_report(true_texts, inference_results)\n",
    "\n",
    "        data['checkpoint'].append(path_to_checkpoint)\n",
    "        data['accuracy_sentences'].append(mean_accuracy_score_by_sentences)\n",
    "        data['f1_characters'].append(mean_f1_score_by_characters)\n",
    "        data['accuracy_characters'].append(mean_accuracy_score_by_characters)\n",
    "\n",
    "        print(f\"mean_accuracy_score_by_sentences :\\t{round(mean_accuracy_score_by_sentences, 4)}\")\n",
    "        print(f\"mean_f1_score_by_characters      :\\t{round(mean_f1_score_by_characters, 4)}\")\n",
    "        print(f\"mean_accuracy_score_by_characters:\\t{round(mean_accuracy_score_by_characters, 4)}\\n\")\n",
    "\n",
    "        for query, result in zip(test_queries, inference_results):\n",
    "            print(f'Query   : {query[0]}')\n",
    "            print(f'Restored: {result.strip()}\\n')\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['f1_characters'], ascending=False)\n",
    "for idx, row in sorted_df.iterrows():\n",
    "    print(row['checkpoint'], row['f1_characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-capitalization\\\\rubert-base-cased-conversational-512-tatoeba_dataset\\\\02-09-2023-11-01-00\\\\checkpoint-7482\")\n",
    "\n",
    "s = [\"в каком году родилась алла пугачёва\"]\n",
    "test = [q[0] for q in test_queries]\n",
    "tic = time.time()\n",
    "inference_results = inference_results = restore_capitalization(test, tokenizer, model)\n",
    "toc = time.time() - tic\n",
    "\n",
    "print(toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
