{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'D:\\\\progamming\\\\va\\\\truecase\\\\ru-punctuation-truecase\\\\src')\n",
    "from process_text import clean_text, clean_text_3times\n",
    "\n",
    "# ========== Data global variables ==========\n",
    "PATH_TO_DATA = \"../data\"\n",
    "\n",
    "# ========== Model global variables ==========\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased-conversational\"\n",
    "# \"DeepPavlov/rubert-base-cased-conversational\" -> rubert-base-cased-conversational\n",
    "SHORT_MODEL_NAME = MODEL_NAME.split('/')[1] if '/' in MODEL_NAME else MODEL_NAME\n",
    "MODEL_MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, tag2id, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, label_names):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['кто', 'такой', 'иван', 'иванов']\n",
    "text_tags = ['U', 'O', 'U', 'U']\n",
    "label2id = {'O': 0, 'U': 1}\n",
    "id2label = {0: 'O', 1: 'U'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "inputs = tokenizer(text_words, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [['один', 'раз', 'в', 'жизни', 'я', 'делаю', 'хорошее', 'дело', 'и', 'оно', 'бесполезно'],\n",
    "        ['кто', 'такой', 'иван', 'иванов']]\n",
    "\n",
    "# text = ['один', 'раз', 'в', 'жизни', 'я', 'делаю', 'хорошее', 'дело', 'и', 'оно', 'бесполезно']\n",
    "\n",
    "text_tags = [['U', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'O', 'O'], \n",
    "             ['U', 'O', 'U', 'U']]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "inputs = tokenizer(text, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "test_labels = encode_tags(text_tags, label2id, inputs)\n",
    "inputs.pop(\"offset_mapping\")\n",
    "\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts_into_words(texts: List[str]):\n",
    "    texts_words = []\n",
    "    for text in texts:\n",
    "        texts_words.append(text.split(' '))\n",
    "    return texts_words\n",
    "\n",
    "\n",
    "def tokenize_texts(texts_words: List[List[str]], tokenizer):\n",
    "    inputs = tokenizer(texts_words, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def predict_token_classification(inputs, token_classification_model):\n",
    "    with torch.no_grad():\n",
    "        logits = token_classification_model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def restore_capitalization(texts: List[str], tokenizer, token_classification_model) -> List[str]:\n",
    "    texts_words = split_texts_into_words(texts)\n",
    "    inputs = tokenize_texts(texts_words, tokenizer)\n",
    "    predictions = predict_token_classification(inputs, token_classification_model)\n",
    "\n",
    "    truecase_texts = []\n",
    "    for text, model_input, predict in zip(texts_words, inputs.encodings, predictions):\n",
    "        predicted_token_class = [model.config.id2label[t.item()] for t in predict]\n",
    "        \n",
    "        word_class = {}\n",
    "        for word_id, token_class in zip(model_input.word_ids, predicted_token_class):\n",
    "            if (word_id != None) and (not word_id in word_class):\n",
    "                word_class[word_id] = token_class\n",
    "        \n",
    "        truecase_words = []\n",
    "        for i, word in enumerate(text):\n",
    "            is_upper = word_class[i] == 'U'\n",
    "            if is_upper:\n",
    "                truecase_word = word.capitalize()\n",
    "            else:\n",
    "                truecase_word = word\n",
    "            \n",
    "            truecase_words.append(truecase_word)\n",
    "\n",
    "        truecase_texts.append(' '.join(truecase_words))\n",
    "    return truecase_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query   : меня зовут сергей а как тебя\n",
      "Combined: Меня зовут Сергей А как тебя\n",
      "\n",
      "Query   : подскажи пожалуйста сегодня вторник или среда\n",
      "Combined: Подскажи пожалуйста сегодня вторник или среда\n",
      "\n",
      "Query   : закрой за мной дверь я ухожу\n",
      "Combined: Закрой за мной дверь я ухожу\n",
      "\n",
      "Query   : в каком году родилась алла пугачёва\n",
      "Combined: В каком году родилась Алла Пугачёва\n",
      "\n",
      "Query   : когда родилась алла пугачёва\n",
      "Combined: Когда родилась алла пугачёва\n",
      "\n",
      "Query   : когда родилась пугачёва\n",
      "Combined: Когда родилась Пугачёва\n",
      "\n",
      "Query   : скажи год рождения пугачевой\n",
      "Combined: Скажи год рождения Пугачевой\n",
      "\n",
      "Query   : год рождения аллы пугачевой\n",
      "Combined: Год рождения Аллы Пугачевой\n",
      "\n",
      "Query   : год рождения пугачевой\n",
      "Combined: Год рождения Пугачевой\n",
      "\n",
      "Query   : день рождения пугачевой\n",
      "Combined: День рождения Пугачевой\n",
      "\n",
      "Query   : когда день рождения у аллы пугачевой\n",
      "Combined: Когда день рождения у Аллы Пугачевой\n",
      "\n",
      "Query   : в каком году родилась алла пугачева\n",
      "Combined: В каком году родилась Алла Пугачева\n",
      "\n",
      "Query   : год рождения пугачевой\n",
      "Combined: Год рождения Пугачевой\n",
      "\n",
      "Query   : иван пятый это кто\n",
      "Combined: Иван пятый это кто\n",
      "\n",
      "Query   : кто такой иван пятый\n",
      "Combined: Кто такой Иван пятый\n",
      "\n",
      "Query   : кто такой иван пятый\n",
      "Combined: Кто такой Иван пятый\n",
      "\n",
      "Query   : что такое лыжи\n",
      "Combined: Что такое лыжи\n",
      "\n",
      "Query   : швеция столица\n",
      "Combined: Швеция столица\n",
      "\n",
      "Query   : столица швеции\n",
      "Combined: Столица швеции\n",
      "\n",
      "Query   : какая столица швеции\n",
      "Combined: Какая столица швеции\n",
      "\n",
      "Query   : как называется столица у швеции\n",
      "Combined: Как называется столица у швеции\n",
      "\n",
      "Query   : расскажи когда родился пушкин\n",
      "Combined: Расскажи когда родился пушкин\n",
      "\n",
      "Query   : когда дата рождения пушкина\n",
      "Combined: Когда дата рождения Пушкина\n",
      "\n",
      "Query   : скажи дату рождения пушкина\n",
      "Combined: Скажи дату рождения Пушкина\n",
      "\n",
      "Query   : сколько прожил пушкин\n",
      "Combined: Сколько прожил Пушкин\n",
      "\n",
      "Query   : когда родился пушкин\n",
      "Combined: Когда родился пушкин\n",
      "\n",
      "Query   : когда родился александр сергеевич пушкин\n",
      "Combined: Когда родился Александр Сергеевич Пушкин\n",
      "\n",
      "Query   : когда родился александр пушкин\n",
      "Combined: Когда родился Александр Пушкин\n",
      "\n",
      "Query   : кто такой лев николаевич толстой\n",
      "Combined: Кто такой Лев николаевич толстой\n",
      "\n",
      "Query   : кто такой лев толстой\n",
      "Combined: Кто такой Лев толстой\n",
      "\n",
      "Query   : сколько лет путину\n",
      "Combined: Сколько лет Путину\n",
      "\n",
      "Query   : возраст путина\n",
      "Combined: Возраст путина\n",
      "\n",
      "Query   : какой возраст у владимира путина\n",
      "Combined: Какой возраст у Владимира путина\n",
      "\n",
      "Query   : какой возраст у путина\n",
      "Combined: Какой возраст у путина\n",
      "\n",
      "Query   : какой возраст у владимира владимировича путина\n",
      "Combined: Какой возраст у Владимира Владимировича путина\n",
      "\n",
      "Query   : праздник благовещение какого числа\n",
      "Combined: Праздник Благовещение какого числа\n",
      "\n",
      "Query   : что такое благовещение пресвятой богородицы\n",
      "Combined: Что такое благовещение Пресвятой богородицы\n",
      "\n",
      "Query   : кому принадлежит компания газпром\n",
      "Combined: Кому принадлежит компания Газпром\n",
      "\n",
      "Query   : кто директор газпрома\n",
      "Combined: Кто директор Газпрома\n",
      "\n",
      "Query   : кто является директором компании газпром\n",
      "Combined: Кто является директором компании газпром\n",
      "\n",
      "Query   : сколько лет москве\n",
      "Combined: Сколько лет москве\n",
      "\n",
      "Query   : сколько лет городу москва\n",
      "Combined: Сколько лет городу Москва\n",
      "\n",
      "Query   : в каком году умер брежнев\n",
      "Combined: В каком году умер брежнев\n",
      "\n",
      "Query   : в каком году скончался леонид брежнев\n",
      "Combined: В каком году скончался Леонид брежнев\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"меня зовут сергей а как тебя\",\n",
    "    \"подскажи пожалуйста сегодня вторник или среда\",\n",
    "    \"закрой за мной дверь я ухожу\",\n",
    "    \"в каком году родилась алла пугачёва\",\n",
    "    \"когда родилась алла пугачёва\",\n",
    "    \"когда родилась пугачёва\",\n",
    "    \"скажи год рождения пугачевой\",\n",
    "    \"год рождения аллы пугачевой\",\n",
    "    \"год рождения пугачевой\",\n",
    "    \"день рождения пугачевой\",\n",
    "    \"когда день рождения у аллы пугачевой\",\n",
    "    \"в каком году родилась алла пугачева\",\n",
    "    \"год рождения пугачевой\",\n",
    "    \"иван пятый это кто\",\n",
    "    \"кто такой иван пятый\",\n",
    "    \"кто такой иван пятый\",\n",
    "    \"что такое лыжи\",\n",
    "    \"швеция столица\",\n",
    "    \"столица швеции\",\n",
    "    \"какая столица швеции\",\n",
    "    \"как называется столица у швеции\",\n",
    "    \"расскажи когда родился пушкин\",\n",
    "    \"когда дата рождения пушкина\",\n",
    "    \"скажи дату рождения пушкина\",\n",
    "    \"сколько прожил пушкин\",\n",
    "    \"когда родился пушкин\",\n",
    "    \"когда родился александр сергеевич пушкин\",\n",
    "    \"когда родился александр пушкин\",\n",
    "    \"кто такой лев николаевич толстой\",\n",
    "    \"кто такой лев толстой\",\n",
    "    \"сколько лет путину\",\n",
    "    \"возраст путина\",\n",
    "    \"какой возраст у владимира путина\",\n",
    "    \"какой возраст у путина\",\n",
    "    \"какой возраст у владимира владимировича путина\",\n",
    "    \"праздник благовещение какого числа\",\n",
    "    \"что такое благовещение пресвятой богородицы\",\n",
    "    \"кому принадлежит компания газпром\",\n",
    "    \"кто директор газпрома\",\n",
    "    \"кто является директором компании газпром\",\n",
    "    \"сколько лет москве\",\n",
    "    \"сколько лет городу москва\",\n",
    "    \"в каком году умер брежнев\",\n",
    "    \"в каком году скончался леонид брежнев\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MODEL_MAX_LENGTH)\n",
    "\n",
    "checkpoint = 'D:/progamming/va/truecase/ru-punctuation-truecase/src/results/rubert-base-cased-conversational-512-tatoeba_dataset/20-53-40/checkpoint-31505/'\n",
    "# checkpoint = \"D:/progamming/va/truecase/ru-punctuation-truecase/src/results/checkpoint-18903/\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "\n",
    "inference_results = restore_capitalization(test_queries, tokenizer, model)\n",
    "for query, result in zip(test_queries, inference_results):\n",
    "    print(f'Query   : {query}')\n",
    "    print(f'Combined: {result.strip()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"D:/progamming/va/truecase/ru-punctuation-truecase/src/results/checkpoint-18903/\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[1]]\n",
    "\n",
    "print(predictions)\n",
    "print(predicted_token_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truecase_texts = []\n",
    "for sentence, model_input, predict in zip(text, inputs.encodings, predictions):\n",
    "    predicted_token_class = [model.config.id2label[t.item()] for t in predict]\n",
    "    \n",
    "    word_class = {}\n",
    "    for word_id, token_class in zip(model_input.word_ids, predicted_token_class):\n",
    "        if (word_id != None) and (not word_id in word_class):\n",
    "            word_class[word_id] = token_class\n",
    "    \n",
    "    truecase_words = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        is_upper = word_class[i] == 'U'\n",
    "        if is_upper:\n",
    "            truecase_word = word.capitalize()\n",
    "        else:\n",
    "            truecase_word = word\n",
    "        \n",
    "        truecase_words.append(truecase_word)\n",
    "\n",
    "    truecase_texts.append(' '.join(truecase_words))\n",
    "truecase_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = inputs[1]\n",
    "sample_predict = predictions[1]\n",
    "sample_input.word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_class = {}\n",
    "for word_id, token_class in zip(sample_input.word_ids, sample_predict):\n",
    "    if (word_id != None) and (not word_id in word_class):\n",
    "        word_class[word_id] = token_class.item()\n",
    "\n",
    "truecase_words = []\n",
    "for i, word in enumerate(text[1]):\n",
    "    is_upper = word_class[i]\n",
    "    if is_upper:\n",
    "        truecase_word = word.capitalize()\n",
    "    else:\n",
    "        truecase_word = word\n",
    "    \n",
    "    truecase_words.append(truecase_word)\n",
    "\n",
    "' '.join(truecase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truecase_text = []\n",
    "for i in range(len(text[1])):\n",
    "    is_upper = word_class[i]\n",
    "    if is_upper:\n",
    "        truecase_word = text[1][i].capitalize()\n",
    "    else:\n",
    "        truecase_word = text[1][i]\n",
    "    \n",
    "    truecase_text.append(truecase_word)\n",
    "\n",
    "' '.join(truecase_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
